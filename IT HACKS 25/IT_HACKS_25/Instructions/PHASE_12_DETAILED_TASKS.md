# PHASE 12 DETAILED TASK BREAKDOWN

## ECO FARM AI - Real-time Monitoring, Anomalies, Explainability, Features

**Project**: ECO FARM AI - IT HACKS 25  
**Phase**: 12  
**Status**: Planning (Ready to Start)  
**Estimated Duration**: 8 hours  
**Start Date**: December 7, 2025

---

## SECTION 1: REAL-TIME MODEL MONITORING DASHBOARD

### 1.1 Backend Monitoring Service Module

**File**: `backend/services/monitoring.py`  
**Size**: 250-300 lines  
**Dependencies**: SQLAlchemy, datetime, numpy

**Classes to Implement**:

```python
class TrainingMetricsCollector:
    """Collect and aggregate training metrics over time"""

    Methods:
    - record_training(model_id, metrics_dict) â†’ None
    - get_training_history(limit=20, offset=0) â†’ List[Dict]
    - get_model_trend(model_id, days=90) â†’ Dict
    - compare_models(model_ids) â†’ Dict
    - calculate_average_metrics(start_date, end_date) â†’ Dict

class PredictionStatsCollector:
    """Collect prediction statistics and latency data"""

    Methods:
    - record_prediction(endpoint, latency_ms, success=True) â†’ None
    - get_prediction_stats(hours=24) â†’ Dict
    - get_latency_histogram(endpoint, hours=24) â†’ Dict
    - calculate_p95_latency(endpoint) â†’ float
    - get_predictions_per_hour(hours=24) â†’ Dict

class SystemHealthMonitor:
    """Monitor system resource usage and health"""

    Methods:
    - get_memory_usage() â†’ Dict
    - get_cpu_usage() â†’ Dict
    - get_disk_usage() â†’ Dict
    - get_model_cache_stats() â†’ Dict
    - get_database_stats() â†’ Dict
    - get_system_status() â†’ Dict (combines all)
```

**Tasks**:

- [ ] Create service module structure
- [ ] Implement TrainingMetricsCollector class
- [ ] Implement PredictionStatsCollector class
- [ ] Implement SystemHealthMonitor class
- [ ] Add database query methods
- [ ] Add caching for expensive operations
- [ ] Write docstrings
- [ ] Create unit tests

---

### 1.2 Backend Monitoring API Endpoints

**File**: `backend/routers/monitoring.py`  
**Size**: 350-400 lines  
**Imports**: FastAPI, monitoring service, database

**Endpoints to Implement**:

```
GET /monitor/training-history
â”œâ”€ Query params: limit=20, offset=0, sort_by="date", order="desc"
â”œâ”€ Returns: Array of training records with:
â”‚  â”œâ”€ version, model_type, trained_at
â”‚  â”œâ”€ metrics: mae, rmse, r2, performance_score
â”‚  â”œâ”€ n_samples, n_features
â”‚  â””â”€ training_time_seconds
â””â”€ Response time target: <300ms

GET /monitor/active-model
â”œâ”€ Returns: Current active model with:
â”‚  â”œâ”€ version, model_type, trained_at
â”‚  â”œâ”€ Current metrics (from last 7 days)
â”‚  â”œâ”€ Performance trend (improving/stable/declining)
â”‚  â””â”€ Next model recommendation (if available)
â””â”€ Response time target: <200ms

GET /monitor/prediction-stats
â”œâ”€ Query params: hours=24, endpoint (optional)
â”œâ”€ Returns: Prediction statistics:
â”‚  â”œâ”€ Total predictions, success rate
â”‚  â”œâ”€ Average latency, p95 latency
â”‚  â”œâ”€ Predictions per hour
â”‚  â””â”€ Error rate by endpoint
â””â”€ Response time target: <400ms

GET /monitor/system-health
â”œâ”€ Returns: Real-time system metrics:
â”‚  â”œâ”€ Memory: used/total/percentage
â”‚  â”œâ”€ CPU: usage, load average
â”‚  â”œâ”€ Database: connections, query time
â”‚  â”œâ”€ Cache: hit rate, size
â”‚  â””â”€ Uptime, last training date
â””â”€ Response time target: <250ms

GET /monitor/model-comparison
â”œâ”€ Query params: limit=5, metric="r2" (mae/rmse/r2)
â”œâ”€ Returns: Top models ranked by metric:
â”‚  â”œâ”€ Model info (version, type, trained_at)
â”‚  â”œâ”€ All metrics (mae, rmse, r2, train_time)
â”‚  â””â”€ Rank and comparison
â””â”€ Response time target: <400ms
```

**Tasks**:

- [ ] Create router module
- [ ] Implement all 5 endpoints
- [ ] Add input validation
- [ ] Add error handling
- [ ] Add RBAC (viewer+ can see)
- [ ] Add caching (5-min TTL)
- [ ] Write endpoint tests
- [ ] Document in OpenAPI

---

### 1.3 Frontend Monitoring Dashboard Page

**File**: `frontend/pages/monitor-dashboard.js`  
**Size**: 200-250 lines  
**Dependencies**: React, Recharts, axios

**Page Layout**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Monitoring Dashboard                  [âŸ³]   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Active Modelâ”‚  â”‚ Avg MAE     â”‚  â”‚ Success    â”‚ â”‚
â”‚  â”‚ v20251207   â”‚  â”‚ 0.0245      â”‚  â”‚ 99.5%      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Training History (Last 7 Days)                 â”‚ â”‚
â”‚  â”‚  [Line Chart: MAE/RMSE trend]                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Model Comparison    â”‚  â”‚ System Health      â”‚  â”‚
â”‚  â”‚ [Bar Chart]         â”‚  â”‚ CPU: 35%           â”‚  â”‚
â”‚  â”‚                     â”‚  â”‚ Memory: 1.8GB      â”‚  â”‚
â”‚  â”‚                     â”‚  â”‚ Cache Hit: 87%     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Prediction Stats (Last 24h)                    â”‚ â”‚
â”‚  â”‚  [Area Chart: Predictions per hour]            â”‚ â”‚
â”‚  â”‚  [Latency: avg 245ms, p95 892ms]              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tasks**:

- [ ] Create page layout structure
- [ ] Set up state management (useState, useEffect)
- [ ] Create data fetching functions
- [ ] Implement auto-refresh (every 10 seconds)
- [ ] Add error boundaries
- [ ] Add loading states
- [ ] Add filtering/sorting
- [ ] Write component tests

---

### 1.4 Frontend Monitoring Components

**Files**: `frontend/components/Monitoring/`

**Components to Create**:

#### TrainingMetrics.js (150 lines)

```
Shows training history as timeline
- Version selector
- Metric cards (MAE, RMSE, RÂ²)
- Trend indicator (â†‘ improving, â†“ declining, â†’ stable)
- Performance score gauge
```

#### ModelComparison.js (150 lines)

```
Compares top 5 models side-by-side
- Bar chart: Model versions vs selected metric
- Metric selector (MAE/RMSE/RÂ²)
- Filter: Last N days
- Switch to "worst performing" view
```

#### SystemHealth.js (100 lines)

```
Shows system resource usage
- Memory gauge (0-2GB)
- CPU usage percentage
- Database connection pool status
- Cache hit rate percentage
- Uptime display
```

#### PredictionStats.js (150 lines)

```
Shows prediction volume and latency
- Line chart: Predictions per hour (24h)
- Latency histogram (response times)
- Success rate gauge
- Error rate by endpoint
- P95/P99 latency display
```

**Tasks**:

- [ ] Create TrainingMetrics component
- [ ] Create ModelComparison component
- [ ] Create SystemHealth component
- [ ] Create PredictionStats component
- [ ] Add real-time updates (polling/websocket)
- [ ] Add interactive legends
- [ ] Add tooltip support
- [ ] Write tests for each component

---

### 1.5 Frontend Integration & Polish

**Tasks**:

- [ ] Add navigation link to /monitor-dashboard
- [ ] Add to main navigation menu
- [ ] Responsive design (mobile-friendly)
- [ ] Dark mode support
- [ ] Export functionality (PNG/CSV)
- [ ] Settings panel (refresh interval, metrics to show)
- [ ] Performance optimization (memoization)
- [ ] Accessibility checks (a11y)

---

### 1.6-1.9 Testing

**Unit Tests** (backend/tests/test_monitoring.py):

- [ ] Test TrainingMetricsCollector methods
- [ ] Test PredictionStatsCollector methods
- [ ] Test SystemHealthMonitor methods
- [ ] Test metric calculations

**API Tests** (backend/tests/test_monitoring_api.py):

- [ ] Test GET /monitor/training-history
- [ ] Test GET /monitor/active-model
- [ ] Test GET /monitor/prediction-stats
- [ ] Test GET /monitor/system-health
- [ ] Test GET /monitor/model-comparison
- [ ] Test response times
- [ ] Test error handling

**Component Tests** (frontend/**tests**/components/):

- [ ] Test TrainingMetrics component
- [ ] Test ModelComparison component
- [ ] Test SystemHealth component
- [ ] Test PredictionStats component
- [ ] Test data fetching
- [ ] Test loading states

**Integration Tests**:

- [ ] Dashboard loads correctly
- [ ] Auto-refresh works
- [ ] Charts render properly
- [ ] No memory leaks
- [ ] Performance acceptable

---

## SECTION 2: ADVANCED ANOMALY DETECTION

### 2.1-2.2 Anomaly Detection Algorithms

**File**: `backend/ml/anomaly_detector_advanced.py`  
**Size**: 400-500 lines

**Classes to Implement**:

```python
class IsolationForestDetector:
    """Multivariate anomaly detection using Isolation Forest"""
    - fit(data) â†’ None
    - predict(data) â†’ array of -1/1 labels
    - anomaly_score(data) â†’ array of 0-1 scores
    - explain_anomaly(sample) â†’ Dict with feature contributions

class LocalOutlierFactorDetector:
    """Density-based anomaly detection"""
    - fit(data) â†’ None
    - predict(data) â†’ array of -1/1 labels
    - anomaly_score(data) â†’ array of 0-1 scores

class StatisticalAnomalyDetector:
    """Univariate Z-score and IQR-based detection"""
    - fit(data) â†’ None
    - detect_by_zscore(data, threshold=3) â†’ List of anomalies
    - detect_by_iqr(data, multiplier=1.5) â†’ List of anomalies
    - get_statistics() â†’ Dict with mean, std, median, IQR

class TimeSeriesAnomalyDetector:
    """Time-series specific anomaly detection"""
    - fit(time_series) â†’ None
    - detect_trend_breaks(data) â†’ List of indices
    - detect_velocity_changes(data, threshold=2) â†’ List of indices
    - detect_seasonal_anomalies(data) â†’ List of indices
    - ARIMA residual analysis
```

**Key Methods**:

- All methods return scores in [0, 1] range
- 0 = normal, 1 = strong anomaly
- Different algorithms detect different patterns
- Combine multiple algorithms for better detection

**Tasks**:

- [ ] Implement IsolationForestDetector
- [ ] Implement LocalOutlierFactorDetector
- [ ] Implement StatisticalAnomalyDetector
- [ ] Implement TimeSeriesAnomalyDetector
- [ ] Add ensemble method (combine algorithms)
- [ ] Add hyperparameter tuning
- [ ] Add documentation
- [ ] Write unit tests

---

### 2.3 Anomaly Detection API Endpoints

**File**: `backend/routers/ai_inference.py` (add to existing)  
**Size**: 100-150 lines

**Endpoints to Add**:

```
GET /ai/anomalies/room/{room_id}
â”œâ”€ Query params: days=7, sensitivity=0.8
â”œâ”€ Returns: Array of anomalies:
â”‚  â”œâ”€ anomaly_date, metric_name, metric_value
â”‚  â”œâ”€ anomaly_score (0-1), anomaly_type
â”‚  â”œâ”€ description, severity (low/medium/high)
â”‚  â””â”€ is_confirmed boolean
â””â”€ Response time target: <1s

GET /ai/anomalies/farm/{farm_id}
â”œâ”€ Query params: days=7, severity (low/medium/high)
â”œâ”€ Returns: Farm-wide anomalies across all rooms
â”‚  â”œâ”€ Room ID, metric, date
â”‚  â”œâ”€ Score and severity
â”‚  â””â”€ Aggregated severity count
â””â”€ Response time target: <2s

POST /ai/anomalies/feedback
â”œâ”€ Body: {
â”‚    "anomaly_id": int,
â”‚    "is_real": boolean,
â”‚    "notes": str (optional)
â”‚  }
â”œâ”€ Purpose: Improve detection with feedback
â””â”€ Response: {"status": "recorded"}
```

**Tasks**:

- [ ] Implement /ai/anomalies/room/{room_id}
- [ ] Implement /ai/anomalies/farm/{farm_id}
- [ ] Implement POST /ai/anomalies/feedback
- [ ] Add input validation
- [ ] Add error handling
- [ ] Add caching (5-min TTL)
- [ ] Add RBAC (viewer+ can see)
- [ ] Write tests

---

### 2.4 Anomaly Database Schema

**File**: `backend/migrations/versions/002_add_anomalies.py`

```sql
CREATE TABLE anomalies (
    id SERIAL PRIMARY KEY,
    room_id INTEGER NOT NULL REFERENCES rooms(id),
    farm_id INTEGER NOT NULL REFERENCES farms(id),
    anomaly_date TIMESTAMP NOT NULL,
    metric_name VARCHAR(100) NOT NULL,
    metric_value FLOAT NOT NULL,
    anomaly_score FLOAT NOT NULL,  -- 0-1
    anomaly_type VARCHAR(50) NOT NULL,
    -- Types: 'multivariate', 'univariate', 'trend', 'contextual'
    description TEXT,
    severity VARCHAR(20) NOT NULL,  -- 'low', 'medium', 'high'
    is_confirmed BOOLEAN DEFAULT FALSE,
    feedback_provided BOOLEAN DEFAULT FALSE,
    user_feedback TEXT,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    FOREIGN KEY (room_id) REFERENCES rooms(id),
    FOREIGN KEY (farm_id) REFERENCES farms(id)
);

CREATE INDEX idx_anomalies_room_date ON anomalies(room_id, anomaly_date DESC);
CREATE INDEX idx_anomalies_farm_date ON anomalies(farm_id, anomaly_date DESC);
CREATE INDEX idx_anomalies_score ON anomalies(anomaly_score DESC);
```

**Tasks**:

- [ ] Create migration file
- [ ] Define anomaly table schema
- [ ] Create indexes
- [ ] Write migration up/down
- [ ] Test migration

---

### 2.5-2.8 Testing

**Unit Tests** (`backend/tests/test_anomaly_detection.py`):

- [ ] Test IsolationForest detection
- [ ] Test LOF detection
- [ ] Test statistical detection
- [ ] Test time-series detection
- [ ] Test ensemble method
- [ ] Test with synthetic anomalies
- [ ] Test score ranges (0-1)

**Integration Tests**:

- [ ] Test detection on real farm data
- [ ] Test API endpoints
- [ ] Test database storage
- [ ] Test feedback mechanism
- [ ] Test sensitivity parameter
- [ ] Test multi-algorithm combination

**Performance Tests**:

- [ ] Detection latency <500ms
- [ ] Memory usage <100MB
- [ ] API response time <2s
- [ ] No memory leaks during streaming data

**Accuracy Tests**:

- [ ] Synthetic anomalies detected >90%
- [ ] False positive rate <5%
- [ ] Real anomalies validated on farm_C, farm_D

---

## SECTION 3: PREDICTION EXPLAINABILITY UI

### 3.1-3.3 Explainability Backend

**File**: `backend/ml/explainability_enhanced.py`  
**Size**: 350-450 lines

**Classes to Implement**:

```python
class SHAPExplainer:
    """SHAP value calculation for model predictions"""

    Methods:
    - __init__(model, training_data)
    - explain_prediction(input_data) â†’ Dict with SHAP values
    - get_feature_importance() â†’ Sorted list of features
    - explain_prediction_batch(data) â†’ List of explanations
    - get_base_value() â†’ float (model baseline)

class FeatureContributionCalculator:
    """Calculate feature contributions to predictions"""

    Methods:
    - calculate_contributions(prediction, features) â†’ Dict
    - get_top_features(n=10) â†’ List of top features
    - get_feature_direction(feature_name) â†’ 'positive'|'negative'
    - get_magnitude(feature_name) â†’ float

class PredictionDecomposer:
    """Decompose prediction into components"""

    Methods:
    - decompose(base_value, features, shap_values) â†’ List
    - generate_waterfall_data(prediction_id) â†’ Dict
    - generate_explanation_text(prediction_id) â†’ str
```

**Key Methods**:

- SHAP values: contribution of each feature
- Must sum to: prediction_value - base_value
- Waterfall chart: show cumulative contributions
- Generate human-readable explanations

**Tasks**:

- [ ] Implement SHAPExplainer class
- [ ] Implement FeatureContributionCalculator
- [ ] Implement PredictionDecomposer
- [ ] Add SHAP value caching (1-hour TTL)
- [ ] Handle edge cases (missing features, etc)
- [ ] Write docstrings
- [ ] Create unit tests

---

### 3.2 Explainability API Endpoints

**File**: `backend/routers/ai_inference.py` (add to existing)

```
GET /ai/explain/prediction/{prediction_id}
â”œâ”€ Returns: Detailed explanation:
â”‚  â”œâ”€ prediction_value, base_value
â”‚  â”œâ”€ shap_values: {
â”‚  â”‚    "feature_1": {"value": float, "contribution": float},
â”‚  â”‚    ...
â”‚  â”‚  }
â”‚  â””â”€ top_features: [
â”‚       {"name": str, "contribution": float, "direction": "pos/neg"}
â”‚     ]
â””â”€ Response time target: <1.5s

GET /ai/explain/prediction/room/{room_id}
â”œâ”€ Query params: metric, days_ahead
â”œâ”€ Returns: Explanation for specific forecast:
â”‚  â”œâ”€ Which metrics drove the prediction
â”‚  â”œâ”€ Top contributing features
â”‚  â””â”€ Waterfall data for visualization
â””â”€ Response time target: <1s

GET /ai/explain/feature-importance
â”œâ”€ Query params: room_id (optional), metric (optional)
â”œâ”€ Returns: Global feature importance:
â”‚  â”œâ”€ Top 10 most important features
â”‚  â”œâ”€ Importance scores (0-1)
â”‚  â””â”€ Trend (stable/increasing/decreasing)
â””â”€ Response time target: <800ms
```

**Tasks**:

- [ ] Implement /ai/explain/prediction/{prediction_id}
- [ ] Implement /ai/explain/prediction/room/{room_id}
- [ ] Implement /ai/explain/feature-importance
- [ ] Add response caching
- [ ] Add error handling
- [ ] Write tests

---

### 3.3 Explainability Data Structure

```json
{
  "prediction_id": 12345,
  "prediction_value": 145.2,
  "prediction_date": "2025-12-07T14:30:00Z",
  "base_value": 130.0,
  "room_id": 5,
  "shap_values": {
    "temperature_c_rolling_7d": {
      "value": 22.5,
      "contribution": 8.2,
      "percentile": 0.65
    },
    "humidity_pct_lag_3d": {
      "value": 65.0,
      "contribution": 4.1,
      "percentile": 0.45
    },
    "weight_trend": {
      "value": 0.15,
      "contribution": 2.9,
      "percentile": 0.8
    }
  },
  "top_features": [
    {
      "name": "temperature_c_rolling_7d",
      "contribution": 8.2,
      "direction": "positive"
    },
    {
      "name": "humidity_pct_lag_3d",
      "contribution": 4.1,
      "direction": "negative"
    },
    { "name": "weight_trend", "contribution": 2.9, "direction": "positive" }
  ],
  "explanation_text": "The prediction of 145.2 eggs is 15.2 higher than baseline due to good temperature conditions (8.2 contribution) and positive weight trend (2.9 contribution)."
}
```

---

### 3.4-3.5 Frontend Explainability Components

**File**: `frontend/components/Explainability/ExplainabilityPanel.js` (250-300 lines)

**Component Display**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Why This Prediction?                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚ Base: 130.0  [=============]  145.2    â”‚
â”‚              Predicted                  â”‚
â”‚                                         â”‚
â”‚ Top Contributing Factors:               â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ 1. Temperature (7d avg)      â†‘ 8.2â”‚   â”‚
â”‚ â”‚    22.5Â°C (Good)              â–ˆâ–ˆâ–ˆ â”‚   â”‚
â”‚ â”‚ 2. Humidity (3d lag)          â†“ 4.1â”‚   â”‚
â”‚ â”‚    65% (Moderate)              â–ˆâ–ˆ â”‚   â”‚
â”‚ â”‚ 3. Weight Trend               â†‘ 2.9â”‚   â”‚
â”‚ â”‚    +0.15 kg/day (Positive)     â–ˆâ–ˆ â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                         â”‚
â”‚ ðŸ“Š Show Waterfall Chart                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tasks**:

- [ ] Create ExplainabilityPanel component
- [ ] Fetch explanation data from API
- [ ] Display top features with contribution bars
- [ ] Show direction indicators (â†‘ pos, â†“ neg)
- [ ] Add color coding (green/red)
- [ ] Generate human-readable text
- [ ] Add waterfall chart toggle
- [ ] Write tests

**Additional Components**:

- `FeatureContributions.js` - Feature list details
- `WaterfallChart.js` - Waterfall visualization

---

### 3.4 Prediction Detail Page

**File**: `frontend/pages/prediction-detail.js` (200-250 lines)

**Page Layout**:

```
Prediction Details: Room 5 - Egg Production
Date: 2025-12-07  |  Metric: Eggs  |  Days Ahead: 7

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prediction: 145.2 eggs                  â”‚
â”‚ Confidence: 95% (135-155 range)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[ Explainability Panel ]

[ Waterfall Chart ]

[ Historical Data Comparison ]
  Previous 7-day: 142.5 eggs (â†‘ +1.9%)
  30-day average: 141.2 eggs
```

**Tasks**:

- [ ] Create page layout
- [ ] Add URL routing (/predictions/:id)
- [ ] Fetch prediction and explanation data
- [ ] Display all components
- [ ] Add historical comparison
- [ ] Add export functionality
- [ ] Write tests

---

### 3.6-3.9 Testing

**Unit Tests**:

- [ ] Test SHAP value calculation
- [ ] Test feature contribution accuracy
- [ ] Test waterfall decomposition
- [ ] Test explanation text generation

**Integration Tests**:

- [ ] Test explanation API endpoints
- [ ] Test with real predictions
- [ ] Test SHAP values sum correctly
- [ ] Test performance <1.5s

**Component Tests**:

- [ ] Test ExplainabilityPanel rendering
- [ ] Test WaterfallChart display
- [ ] Test data loading states
- [ ] Test error handling

---

## SECTION 4: FEATURE IMPORTANCE VISUALIZATION

### 4.1-4.3 Feature Importance Service

**File**: `backend/services/feature_importance.py` (200-250 lines)

**Classes to Implement**:

```python
class FeatureImportanceTracker:
    """Track feature importance over time"""

    Methods:
    - record_importance(model_id, feature_name, score, room_id=None)
    - get_feature_importance(model_id, room_id=None) â†’ DataFrame
    - get_importance_trend(feature_name, days=90) â†’ List
    - calculate_importance_stability(feature_name) â†’ float
    - compare_rooms(feature_name, room_ids) â†’ Dict
    - get_seasonal_importance(feature_name) â†’ Dict
```

**Database Schema**:

```sql
CREATE TABLE feature_importance (
    id SERIAL PRIMARY KEY,
    model_id INTEGER NOT NULL REFERENCES ml_models(id),
    feature_name VARCHAR(100) NOT NULL,
    importance_score FLOAT NOT NULL,  -- 0-1
    rank INTEGER,
    room_id INTEGER REFERENCES rooms(id),  -- NULL for global
    calculated_date TIMESTAMP NOT NULL,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_feature_importance_model ON feature_importance(model_id);
CREATE INDEX idx_feature_importance_date ON feature_importance(calculated_date DESC);
CREATE INDEX idx_feature_importance_room ON feature_importance(room_id);
```

**Tasks**:

- [ ] Create FeatureImportanceTracker class
- [ ] Implement recording methods
- [ ] Implement retrieval methods
- [ ] Add trend calculation
- [ ] Add stability calculation
- [ ] Write docstrings
- [ ] Create unit tests

---

### 4.2 Feature Importance API Endpoints

**File**: `backend/routers/monitoring.py` (add to existing)

```
GET /monitor/feature-importance
â”œâ”€ Query params: room_id (optional), model_id (optional), limit=20
â”œâ”€ Returns: Top features with scores:
â”‚  â”œâ”€ feature_name, importance_score (0-1)
â”‚  â”œâ”€ rank (1-100), trend (â†‘/â†’/â†“)
â”‚  â””â”€ Global or room-specific
â””â”€ Response time target: <500ms

GET /monitor/feature-importance/history
â”œâ”€ Query params: feature_name, days=90
â”œâ”€ Returns: Importance trend over time:
â”‚  â”œâ”€ Array of {date, importance_score}
â”‚  â”œâ”€ Moving average (7-day)
â”‚  â””â”€ Trend line (linear fit)
â””â”€ Response time target: <800ms

GET /monitor/feature-importance/comparison
â”œâ”€ Query params: room_id_1, room_id_2
â”œâ”€ Returns: Side-by-side feature comparison:
â”‚  â”œâ”€ Top 15 features for each room
â”‚  â”œâ”€ Importance scores compared
â”‚  â””â”€ Differences highlighted
â””â”€ Response time target: <600ms

GET /monitor/feature-importance/seasonal
â”œâ”€ Returns: Feature importance by season:
â”‚  â”œâ”€ Spring, Summer, Fall, Winter
â”‚  â”œâ”€ Top 10 features per season
â”‚  â””â”€ Seasonal variations
â””â”€ Response time target: <700ms
```

**Tasks**:

- [ ] Implement /monitor/feature-importance
- [ ] Implement /monitor/feature-importance/history
- [ ] Implement /monitor/feature-importance/comparison
- [ ] Implement /monitor/feature-importance/seasonal
- [ ] Add caching (10-min TTL)
- [ ] Add error handling
- [ ] Write tests

---

### 4.4-4.6 Frontend Feature Importance Components

**Component**: `frontend/components/FeatureImportance/ImportanceChart.js` (150 lines)

**Displays**: Horizontal bar chart of top 20 features

```
Top Features by Importance

temperature_c_rolling_7d    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ 0.85
humidity_pct_lag_3d          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.52
weight_trend                 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.42
eggs_produced                â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.37
mortality_rate               â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.33
[... more features ...]
```

**Tasks**:

- [ ] Create ImportanceChart component
- [ ] Fetch data from API
- [ ] Render horizontal bar chart
- [ ] Add hover tooltips
- [ ] Color code by importance

**Component**: `frontend/components/FeatureImportance/ImportanceTrend.js` (150 lines)

**Displays**: Line chart showing how importance changes over time

```
Feature Importance Over 90 Days

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 0.9 â”‚  â•±â•²                          â”‚
â”‚ 0.8 â”‚ â•±  â•²    â•±â•²    â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚ 0.7 â”‚â•±    â•²â•­â”€â•¯  â•°â•® â•±              â”‚
â”‚ 0.6 â”‚      â•±      â•²â•±               â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚     Oct 1        Nov 1        Dec 1â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tasks**:

- [ ] Create ImportanceTrend component
- [ ] Fetch historical data from API
- [ ] Render line chart with trend
- [ ] Add moving average overlay
- [ ] Add date range selector

**Component**: `frontend/components/FeatureImportance/RoomComparison.js` (150 lines)

**Displays**: Heatmap comparing features across rooms

**Page**: `frontend/pages/features.js` (300-350 lines)

**Layout**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature Importance Analysis             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [Room Selector] [Time Period] [Export]  â”‚
â”‚                                         â”‚
â”‚ Global Top Features                     â”‚
â”‚ [ ImportanceChart ]                     â”‚
â”‚                                         â”‚
â”‚ Importance Trends (90 days)             â”‚
â”‚ [ ImportanceTrend ]                     â”‚
â”‚                                         â”‚
â”‚ Room Comparison                         â”‚
â”‚ [ RoomComparison Heatmap ]              â”‚
â”‚                                         â”‚
â”‚ Seasonal Analysis                       â”‚
â”‚ Spring  Summer  Fall  Winter            â”‚
â”‚ [Seasonal Bar Charts]                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tasks**:

- [ ] Create features.js page
- [ ] Add room selector
- [ ] Add time period selector
- [ ] Integrate all components
- [ ] Add export functionality
- [ ] Responsive design
- [ ] Write tests

---

### 4.7-4.11 Visualization & Testing

**Visualization Types** (using Recharts):

1. **Horizontal Bar Chart**

   - Features on Y-axis, importance on X-axis
   - Top 20 features
   - Animated bars

2. **Line Chart** (Trends)

   - Time on X-axis, importance on Y-axis
   - Multiple features with legend
   - Hover tooltips

3. **Heatmap** (Room Comparison)

   - Rooms on Y-axis, features on X-axis
   - Color intensity = importance
   - Cells show exact values

4. **Pie Chart** (Dominance)
   - Top 5 features vs Others
   - Show percentage

**Testing Tasks**:

- [ ] Unit tests for importance calculation
- [ ] Integration tests with real models
- [ ] Component rendering tests
- [ ] Chart accuracy tests
- [ ] Performance tests (<1s API)
- [ ] Real data validation

---

## FINAL INTEGRATION CHECKLIST

### 4.8 Integration with Main App

**Updates to Existing Files**:

- [ ] Update `frontend/components/Navbar.js` - Add links to new pages
- [ ] Update `frontend/pages/_app.js` - Add new routes
- [ ] Update `backend/main.py` - Register new routers
- [ ] Update database migrations - Run all new schemas
- [ ] Update documentation index

### 4.9 Database Migrations

- [ ] Create migration: 002_add_anomalies.py
- [ ] Create migration: 003_add_feature_importance.py
- [ ] Test migrations up/down
- [ ] Backup production DB before running

### 4.10 Performance Testing

- [ ] Load test monitoring endpoints
- [ ] Load test anomaly detection
- [ ] Load test explainability
- [ ] Load test feature importance
- [ ] Verify all response times <2s
- [ ] Check memory usage

### 4.11 Documentation

- [ ] Write user guide for monitoring dashboard
- [ ] Write API documentation for new endpoints
- [ ] Document anomaly detection algorithm
- [ ] Document SHAP explainability
- [ ] Create deployment guide

---

## SUCCESS METRICS

**Phase 12 is complete when:**

âœ… All 4 sections implemented and tested  
âœ… 1000+ lines of code added  
âœ… 100% test coverage  
âœ… All API endpoints respond in <2s  
âœ… Frontend components render smoothly  
âœ… Documentation complete  
âœ… Zero critical bugs  
âœ… Real data tested (farm_C, farm_D)

---

**Estimated Total Time**: 8 hours  
**Start Date**: December 7, 2025  
**Target Completion**: December 8, 2025 (continuous)

---

_This detailed breakdown is ready to use as-is. Start with Section 1, complete all tasks, commit, then move to Section 2. One section at a time._
